{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from utils import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import train_visualization, weight_initialization, shuffle\n",
    "from torch.nn import functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 3],\n",
       "        [5, 4],\n",
       "        [7, 4],\n",
       "        ...,\n",
       "        [1, 4],\n",
       "        [3, 5],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes\n",
    "concat = torch.zeros((1000,20))\n",
    "\n",
    "for idx, (tgt1,tgt2)  in enumerate(train_classes): \n",
    "    \n",
    "    concat[idx,tgt1] = 1\n",
    "    concat[idx,10+tgt2] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.ones((train_classes.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas utile ... \n",
    "for i, (pair1,pair2) in enumerate(train_classes): \n",
    "    target[i] = torch.tensor([1,0]) if pair1 > pair2 else  torch.tensor([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        ## Train pour mnist classificiation\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=5,padding=1) # 1 filter 32 channels , 5x5\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "                              \n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "        self.fc2 = nn.Linear(200, 1) # train_class cross + train_class \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## train pour reconnaitre \n",
    "        \n",
    "        #self.fc3 = nn.Linear(20, 1) # decision layer \n",
    "        print(f'Parameters : {self.count_params()}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('input',x.shape)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=2))\n",
    "        #print('conv1',x.shape)\n",
    "\n",
    "\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2,stride = 1))\n",
    "        \n",
    "        #print('relu',x.shape)\n",
    "\n",
    "\n",
    "        #print('conv2',x.shape)\n",
    "        #print('ef',x.view(-1,256).shape)\n",
    "        x = F.relu(self.fc1(x.view(-1,256)))\n",
    "        x= self.drop(x)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "       # print(x.shape)\n",
    "        x= self.drop(x)\n",
    "        \n",
    "        #out1 = x\n",
    "        \n",
    "        #out2 = F.sigmoid(self.fc3(out1))\n",
    "\n",
    "        return x.flatten()\n",
    "        \n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters()if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(net, train,target):\n",
    "    with torch.no_grad():\n",
    "        out1 = net(train)\n",
    "        #max_ = out1.argmax(1)\n",
    "\n",
    "        for i, x in enumerate(out1):\n",
    "            out1[i] = 1 if x > 0.5 else 0 \n",
    "    return (out1 == target).sum().item()/len(target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_input, train_target, train_classes, eta=1e-3, n_epochs=25, batch_size=100, verbose=True):\n",
    "    criterion1 = nn.BCELoss()\n",
    "    criterion2 = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=eta, weight_decay=1e-5)\n",
    "    \n",
    "    losses, accuracies = [], []\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        sum_loss = 0\n",
    "        loss_auxs=0\n",
    "        net.train()\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            trainX = train_input.narrow(0, b, batch_size)\n",
    "            trainC = train_target.narrow(0, b, batch_size)\n",
    "            trainT = concat.narrow(0, b, batch_size)\n",
    "            \n",
    "\n",
    "            final = net(trainX)\n",
    "            \n",
    "            #print(out1.shape,trainC.shape)\n",
    "            \n",
    "            \n",
    "            #loss_aux = criterion2(aux,trainT.float())\n",
    "            \n",
    "           # print(trainX.shape)\n",
    "            \n",
    "            loss_final = criterion1(final,trainC.float())\n",
    "            \n",
    "            \n",
    "            loss =   loss_final\n",
    "            \n",
    "            \n",
    "            sum_loss += loss.item()\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            acc = compute_accuracy(net, train_input, train_target)\n",
    "        losses.append(sum_loss)\n",
    "        accuracies.append(acc)\n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Loss: %.3f, Auxiliary: %.3f,  Accuracy: %.3f' % (e+1, n_epochs, sum_loss,0,acc))\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "def trial(net, train_input, train_classes, train_target, test_input, test_target, n_trials=10):\n",
    "    all_losses = []\n",
    "    all_train_accuracies = []\n",
    "    all_test_accuracies = []\n",
    "    for i in range(n_trials):\n",
    "        print(f'Trial {i+1}/{n_trials}...')\n",
    "        # Shuffle data\n",
    "        train_input, train_target, train_classes = shuffle(train_input, train_target, train_classes)\n",
    "        \n",
    "        # Reset weights\n",
    "        net.train()\n",
    "        net.apply(weight_initialization)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train(net, train_input, train_target, train_classes, verbose=False)\n",
    "        \n",
    "        # Collect data\n",
    "        all_losses.append(train_loss)\n",
    "        all_train_accuracies.append(train_acc)\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_acc = compute_accuracy(net, test_input, test_target)\n",
    "            all_test_accuracies.append(test_acc)\n",
    "        \n",
    "        print('Loss: %.3f, Train accuracy: %.3f, Test_accuracy: %.3f' % (train_loss[-1], train_acc[-1], test_acc))\n",
    "    \n",
    "    return torch.FloatTensor(all_losses), torch.FloatTensor(all_train_accuracies), torch.FloatTensor(all_test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : 71729\n"
     ]
    }
   ],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.639, Auxiliary: 0.000,  Accuracy: 0.717\n",
      "Epoch 2/25, Loss: 5.718, Auxiliary: 0.000,  Accuracy: 0.743\n",
      "Epoch 3/25, Loss: 5.022, Auxiliary: 0.000,  Accuracy: 0.769\n",
      "Epoch 4/25, Loss: 4.511, Auxiliary: 0.000,  Accuracy: 0.809\n",
      "Epoch 5/25, Loss: 4.083, Auxiliary: 0.000,  Accuracy: 0.822\n",
      "Epoch 6/25, Loss: 3.705, Auxiliary: 0.000,  Accuracy: 0.838\n",
      "Epoch 7/25, Loss: 3.387, Auxiliary: 0.000,  Accuracy: 0.852\n",
      "Epoch 8/25, Loss: 3.097, Auxiliary: 0.000,  Accuracy: 0.883\n",
      "Epoch 9/25, Loss: 2.836, Auxiliary: 0.000,  Accuracy: 0.898\n",
      "Epoch 10/25, Loss: 2.644, Auxiliary: 0.000,  Accuracy: 0.883\n",
      "Epoch 11/25, Loss: 2.532, Auxiliary: 0.000,  Accuracy: 0.926\n",
      "Epoch 12/25, Loss: 2.176, Auxiliary: 0.000,  Accuracy: 0.906\n",
      "Epoch 13/25, Loss: 1.875, Auxiliary: 0.000,  Accuracy: 0.947\n",
      "Epoch 14/25, Loss: 1.597, Auxiliary: 0.000,  Accuracy: 0.964\n",
      "Epoch 15/25, Loss: 1.408, Auxiliary: 0.000,  Accuracy: 0.957\n",
      "Epoch 16/25, Loss: 1.299, Auxiliary: 0.000,  Accuracy: 0.959\n",
      "Epoch 17/25, Loss: 1.248, Auxiliary: 0.000,  Accuracy: 0.984\n",
      "Epoch 18/25, Loss: 1.219, Auxiliary: 0.000,  Accuracy: 0.898\n",
      "Epoch 19/25, Loss: 1.334, Auxiliary: 0.000,  Accuracy: 0.988\n",
      "Epoch 20/25, Loss: 0.931, Auxiliary: 0.000,  Accuracy: 0.981\n",
      "Epoch 21/25, Loss: 0.741, Auxiliary: 0.000,  Accuracy: 0.995\n",
      "Epoch 22/25, Loss: 0.502, Auxiliary: 0.000,  Accuracy: 0.997\n",
      "Epoch 23/25, Loss: 0.483, Auxiliary: 0.000,  Accuracy: 0.999\n",
      "Epoch 24/25, Loss: 0.352, Auxiliary: 0.000,  Accuracy: 0.998\n",
      "Epoch 25/25, Loss: 0.350, Auxiliary: 0.000,  Accuracy: 0.998\n",
      "6.158755779266357\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train(net, train_input, train_target, train_classes, batch_size=100)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10...\n",
      "Loss: 0.078, Train accuracy: 1.000, Test_accuracy: 0.807\n",
      "Trial 2/10...\n",
      "Loss: 0.055, Train accuracy: 1.000, Test_accuracy: 0.817\n",
      "Trial 3/10...\n",
      "Loss: 0.031, Train accuracy: 1.000, Test_accuracy: 0.814\n",
      "Trial 4/10...\n",
      "Loss: 1.371, Train accuracy: 0.997, Test_accuracy: 0.800\n",
      "Trial 5/10...\n",
      "Loss: 0.632, Train accuracy: 0.985, Test_accuracy: 0.802\n",
      "Trial 6/10...\n",
      "Loss: 0.201, Train accuracy: 1.000, Test_accuracy: 0.820\n",
      "Trial 7/10...\n",
      "Loss: 1.935, Train accuracy: 1.000, Test_accuracy: 0.820\n",
      "Trial 8/10...\n",
      "Loss: 0.001, Train accuracy: 1.000, Test_accuracy: 0.825\n",
      "Trial 9/10...\n",
      "Loss: 0.001, Train accuracy: 1.000, Test_accuracy: 0.822\n",
      "Trial 10/10...\n",
      "Loss: 0.001, Train accuracy: 1.000, Test_accuracy: 0.814\n",
      "61.48161697387695\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tr_loss, tr_acc, te_acc = trial(net, train_input, train_classes, train_target, test_input, test_target)\n",
    "\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : 71729\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (10) must match the existing size (100) at non-singleton dimension 1.  Target sizes: [100, 10].  Tensor sizes: [100]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-a437780ae518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-a437780ae518>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, mini_batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (10) must match the existing size (100) at non-singleton dimension 1.  Target sizes: [100, 10].  Tensor sizes: [100]"
     ]
    }
   ],
   "source": [
    "\n",
    "model, criterion = Net(), nn.MSELoss()\n",
    "eta, mini_batch_size = 1e-1, 100\n",
    "\n",
    "def train_model(model, train_input, train_target, mini_batch_size): \n",
    "    loss_ = []\n",
    "    out = torch.empty((25,1000,10))\n",
    "    for e in range(0, 25):\n",
    "        sum_loss = 0\n",
    "    # We do this with mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            out[e,b:b+100] = output\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= eta * p.grad\n",
    "        loss_.append(sum_loss)\n",
    "    return loss_, out , model \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "losses,_,model_ = train_model(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.ones((100,64,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 256]' is invalid for input of size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-dbe9f0d26059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[100, 256]' is invalid for input of size 100"
     ]
    }
   ],
   "source": [
    "x.reshape(100,256).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.ones([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=torch.ones([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x==x1).sum(0).item()/len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
