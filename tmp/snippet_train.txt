dataloader = torch.utils.data.DataLoader(train_dataset,
                                             batch_size=batch_size)
    optimizer = optim(model.parameters(), **optim_kwargs)

    losses = np.full((epochs,), np.nan)
    for epoch_idx in range(epochs):
        for batch_idx, (x, y) in enumerate(dataloader):
            y_pred = model.forward(x)
            loss = criterion(y_pred, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        losses[epoch_idx] = loss.item()
        if epoch_idx % log_every_epoch == 0:
            logger.info("Epoch completed {:4d} / {:4d} -- loss = {:4f}".format(epoch_idx, epochs, losses[epoch_idx]))
