{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from utils import load_data, split_data, shuffle, weight_initialization\n",
    "from metrics import compute_accuracy\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data()\n",
    "train_loader, valid_loader, test_loader = split_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(net, train_loader, test_loader, n_epochs=25, n_trials=20, alpha=1, alpha_decay=1, verbose=True):\n",
    "    all_losses = torch.zeros((n_trials, n_epochs))\n",
    "    tr_accuracies = torch.zeros(n_trials)\n",
    "    te_accuracies = torch.zeros(n_trials)\n",
    "    for i in range(n_trials):\n",
    "        # Shuffle data\n",
    "        train_loader, valid_loader, test_loader = split_data(train_data, test_data)\n",
    "        \n",
    "        # Reset weights\n",
    "        net.train()\n",
    "        net.apply(weight_initialization)\n",
    "        \n",
    "        # Train\n",
    "        start = time.time()\n",
    "        train_loss = train(net, train_loader, alpha=alpha, alpha_decay=alpha_decay, verbose=False)\n",
    "        print('Trial %d/%d... Training time: %.2f s' % (i+1, n_trials, time.time()-start))\n",
    "        \n",
    "        # Collect data\n",
    "        all_losses[i] = train_loss\n",
    "        \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            tr_accuracies[i] = compute_accuracy(net, train_loader)\n",
    "            te_accuracies[i] = compute_accuracy(net, test_loader)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Loss: %.3f, Train acc: %.3f, Test acc: %.3f' % \n",
    "                  (train_loss[-1], tr_accuracies[i], te_accuracies[i]))\n",
    "    \n",
    "    return all_losses, tr_accuracies, te_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, eta=1e-3, decay=1e-5, n_epochs=25, alpha=1, alpha_decay=1, verbose=True):\n",
    "    aux_crit = nn.CrossEntropyLoss()\n",
    "    binary_crit = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=eta, weight_decay=decay)\n",
    "    #optimizer = optim.RMSprop(net.parameters(), lr=eta, weight_decay=decay)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=eta, weight_decay=decay, momentum=0.9, nesterov=True)\n",
    "    #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "    tr_losses, val_losses = torch.zeros(n_epochs), torch.zeros(n_epochs)\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        tr_loss, val_loss, aux_loss = 0, 0, 0\n",
    "        net.train()\n",
    "        \n",
    "        for (trainX, trainY, trainC) in train_loader:\n",
    "            out, aux = net(trainX)\n",
    "            \n",
    "            aux1, aux2 = aux.unbind(1)\n",
    "            c1, c2 = trainC.unbind(1)\n",
    "            \n",
    "            aux_loss = aux_crit(aux1, c1) + aux_crit(aux2, c2)\n",
    "\n",
    "            binary_loss = binary_crit(out, trainY.float())\n",
    "            \n",
    "            total_loss = binary_loss + alpha*aux_loss\n",
    "            \n",
    "            tr_loss += total_loss.item()\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (valX, valY, trainC) in valid_loader:\n",
    "                out, _ = net(valX)\n",
    "                loss = binary_crit(out, valY.float())\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        tr_losses[e] = tr_loss\n",
    "        val_losses[e] = val_loss\n",
    "                \n",
    "        #scheduler.step(val_loss)\n",
    "        alpha *= alpha_decay\n",
    "        \n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Train loss: %.3f, Val loss: %.3f, Aux loss: %.3f' % \n",
    "                  (e+1, n_epochs, tr_loss, val_loss, aux_loss))\n",
    "    \n",
    "    return tr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden=128, verbose=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(14*14, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, 10)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "       \n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Parameters: {self.count_params()}')\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def forward(self, x):   \n",
    "        x = self.relu(self.fc1(x.flatten(start_dim=2)))\n",
    "        #x = self.drop(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        #x = self.drop(x)\n",
    "        \n",
    "        aux = x\n",
    "\n",
    "        x = self.relu(self.fc3(x.flatten(start_dim=1)))\n",
    "        #x = self.drop(x)\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x.squeeze(), aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, verbose=True):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 2, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(16, 10)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Parameters: {self.count_params()}')\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def forward(self, x):   \n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        aux = self.relu(self.conv3(x))\n",
    "        aux = self.fc1(aux.flatten(start_dim=2))\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = self.drop(self.relu(self.fc2(x.flatten(start_dim=1))))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x.squeeze(), aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = None\n",
    "for i, (img, lab, clas) in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        tt = img\n",
    "cnn = CNN(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 26727\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 2, 128]          25,216\n",
      "              ReLU-2               [-1, 2, 128]               0\n",
      "            Linear-3                [-1, 2, 10]           1,290\n",
      "              ReLU-4                [-1, 2, 10]               0\n",
      "            Linear-5                   [-1, 10]             210\n",
      "              ReLU-6                   [-1, 10]               0\n",
      "            Linear-7                    [-1, 1]              11\n",
      "           Sigmoid-8                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 26,727\n",
      "Trainable params: 26,727\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 0.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "summary(net, (2, 14, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train loss: 78.253, Val loss: 2.783, Aux loss: 3.645\n",
      "Epoch 2/25, Train loss: 60.841, Val loss: 2.738, Aux loss: 2.767\n",
      "Epoch 3/25, Train loss: 49.049, Val loss: 2.670, Aux loss: 2.002\n",
      "Epoch 4/25, Train loss: 44.561, Val loss: 2.583, Aux loss: 2.211\n",
      "Epoch 5/25, Train loss: 41.374, Val loss: 2.410, Aux loss: 1.904\n",
      "Epoch 6/25, Train loss: 39.542, Val loss: 2.402, Aux loss: 2.080\n",
      "Epoch 7/25, Train loss: 37.301, Val loss: 2.305, Aux loss: 1.874\n",
      "Epoch 8/25, Train loss: 35.385, Val loss: 2.222, Aux loss: 1.858\n",
      "Epoch 9/25, Train loss: 33.182, Val loss: 2.083, Aux loss: 1.584\n",
      "Epoch 10/25, Train loss: 33.296, Val loss: 2.040, Aux loss: 1.797\n",
      "Epoch 11/25, Train loss: 30.788, Val loss: 1.853, Aux loss: 1.702\n",
      "Epoch 12/25, Train loss: 29.505, Val loss: 1.875, Aux loss: 1.545\n",
      "Epoch 13/25, Train loss: 29.805, Val loss: 1.828, Aux loss: 1.692\n",
      "Epoch 14/25, Train loss: 27.893, Val loss: 1.835, Aux loss: 1.272\n",
      "Epoch 15/25, Train loss: 27.921, Val loss: 1.751, Aux loss: 1.590\n",
      "Epoch 16/25, Train loss: 26.883, Val loss: 1.704, Aux loss: 1.724\n",
      "Epoch 17/25, Train loss: 26.553, Val loss: 1.807, Aux loss: 1.573\n",
      "Epoch 18/25, Train loss: 24.684, Val loss: 1.735, Aux loss: 1.402\n",
      "Epoch 19/25, Train loss: 25.139, Val loss: 1.695, Aux loss: 1.278\n",
      "Epoch 20/25, Train loss: 24.920, Val loss: 1.626, Aux loss: 1.254\n",
      "Epoch 21/25, Train loss: 24.549, Val loss: 1.794, Aux loss: 1.561\n",
      "Epoch 22/25, Train loss: 22.979, Val loss: 1.758, Aux loss: 1.147\n",
      "Epoch 23/25, Train loss: 23.029, Val loss: 1.411, Aux loss: 1.099\n",
      "Epoch 24/25, Train loss: 22.650, Val loss: 1.785, Aux loss: 1.316\n",
      "Epoch 25/25, Train loss: 23.590, Val loss: 1.622, Aux loss: 1.313\n",
      "4.771998167037964\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "net = Net(verbose=False)\n",
    "tr_losses = train(net, train_loader, alpha=1, alpha_decay=.99, n_epochs=25)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train loss: 10.870, Val loss: 2.736, Aux loss: 4.617\n",
      "Epoch 2/25, Train loss: 10.130, Val loss: 2.418, Aux loss: 4.620\n",
      "Epoch 3/25, Train loss: 8.936, Val loss: 2.205, Aux loss: 4.592\n",
      "Epoch 4/25, Train loss: 7.899, Val loss: 2.045, Aux loss: 4.595\n",
      "Epoch 5/25, Train loss: 7.131, Val loss: 1.839, Aux loss: 4.610\n",
      "Epoch 6/25, Train loss: 6.248, Val loss: 1.908, Aux loss: 4.605\n",
      "Epoch 7/25, Train loss: 5.639, Val loss: 1.821, Aux loss: 4.605\n",
      "Epoch 8/25, Train loss: 5.108, Val loss: 1.587, Aux loss: 4.596\n",
      "Epoch 9/25, Train loss: 4.428, Val loss: 1.683, Aux loss: 4.612\n",
      "Epoch 10/25, Train loss: 3.264, Val loss: 1.708, Aux loss: 4.602\n",
      "Epoch 11/25, Train loss: 2.886, Val loss: 1.906, Aux loss: 4.602\n",
      "Epoch 12/25, Train loss: 2.251, Val loss: 1.825, Aux loss: 4.603\n",
      "Epoch 13/25, Train loss: 1.639, Val loss: 1.959, Aux loss: 4.601\n",
      "Epoch 14/25, Train loss: 1.273, Val loss: 1.843, Aux loss: 4.606\n",
      "Epoch 15/25, Train loss: 0.899, Val loss: 2.019, Aux loss: 4.605\n",
      "Epoch 16/25, Train loss: 0.602, Val loss: 2.114, Aux loss: 4.605\n",
      "Epoch 17/25, Train loss: 0.424, Val loss: 2.341, Aux loss: 4.604\n",
      "Epoch 18/25, Train loss: 0.378, Val loss: 2.324, Aux loss: 4.606\n",
      "Epoch 19/25, Train loss: 0.263, Val loss: 2.406, Aux loss: 4.605\n",
      "Epoch 20/25, Train loss: 0.196, Val loss: 2.585, Aux loss: 4.604\n",
      "Epoch 21/25, Train loss: 0.166, Val loss: 2.620, Aux loss: 4.605\n",
      "Epoch 22/25, Train loss: 0.138, Val loss: 2.579, Aux loss: 4.605\n",
      "Epoch 23/25, Train loss: 0.124, Val loss: 2.609, Aux loss: 4.605\n",
      "Epoch 24/25, Train loss: 0.090, Val loss: 2.724, Aux loss: 4.605\n",
      "Epoch 25/25, Train loss: 0.080, Val loss: 3.067, Aux loss: 4.605\n",
      "33.30100107192993\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "net=CNN(verbose=False)\n",
    "tr_losses = train(net, train_loader, alpha=0, alpha_decay=.99, eta=1e-3, decay=1e-5)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/30... Training time: 3.60 s\n",
      "Loss: 1.062, Train acc: 0.993, Test acc: 0.803\n",
      "Trial 2/30... Training time: 3.93 s\n",
      "Loss: 0.708, Train acc: 0.998, Test acc: 0.816\n",
      "Trial 3/30... Training time: 3.51 s\n",
      "Loss: 1.172, Train acc: 0.989, Test acc: 0.819\n",
      "Trial 4/30... Training time: 3.48 s\n",
      "Loss: 0.773, Train acc: 0.996, Test acc: 0.812\n",
      "Trial 5/30... Training time: 3.52 s\n",
      "Loss: 1.259, Train acc: 0.989, Test acc: 0.813\n",
      "Trial 6/30... Training time: 3.60 s\n",
      "Loss: 1.311, Train acc: 0.988, Test acc: 0.812\n",
      "Trial 7/30... Training time: 3.34 s\n",
      "Loss: 1.413, Train acc: 0.979, Test acc: 0.794\n",
      "Trial 8/30... Training time: 3.63 s\n",
      "Loss: 0.888, Train acc: 0.998, Test acc: 0.822\n",
      "Trial 9/30... Training time: 3.64 s\n",
      "Loss: 1.167, Train acc: 0.993, Test acc: 0.810\n",
      "Trial 10/30... Training time: 3.49 s\n",
      "Loss: 0.811, Train acc: 0.994, Test acc: 0.813\n",
      "Trial 11/30... Training time: 3.83 s\n",
      "Loss: 0.574, Train acc: 0.999, Test acc: 0.828\n",
      "Trial 12/30... Training time: 4.49 s\n",
      "Loss: 0.349, Train acc: 1.000, Test acc: 0.823\n",
      "Trial 13/30... Training time: 4.35 s\n",
      "Loss: 1.216, Train acc: 0.990, Test acc: 0.812\n",
      "Trial 14/30... Training time: 4.53 s\n",
      "Loss: 1.100, Train acc: 0.993, Test acc: 0.820\n",
      "Trial 15/30... Training time: 4.98 s\n",
      "Loss: 0.836, Train acc: 0.996, Test acc: 0.823\n",
      "Trial 16/30... Training time: 3.87 s\n",
      "Loss: 0.886, Train acc: 0.996, Test acc: 0.816\n",
      "Trial 17/30... Training time: 3.67 s\n",
      "Loss: 0.591, Train acc: 0.998, Test acc: 0.806\n",
      "Trial 18/30... Training time: 3.96 s\n",
      "Loss: 0.694, Train acc: 0.999, Test acc: 0.804\n",
      "Trial 19/30... Training time: 4.02 s\n",
      "Loss: 0.787, Train acc: 0.995, Test acc: 0.814\n",
      "Trial 20/30... Training time: 3.31 s\n",
      "Loss: 0.573, Train acc: 0.996, Test acc: 0.809\n",
      "Trial 21/30... Training time: 3.39 s\n",
      "Loss: 1.294, Train acc: 0.990, Test acc: 0.811\n",
      "Trial 22/30... Training time: 3.17 s\n",
      "Loss: 0.797, Train acc: 0.994, Test acc: 0.814\n",
      "Trial 23/30... Training time: 4.53 s\n",
      "Loss: 0.760, Train acc: 0.999, Test acc: 0.821\n",
      "Trial 24/30... Training time: 5.20 s\n",
      "Loss: 1.498, Train acc: 0.981, Test acc: 0.811\n",
      "Trial 25/30... Training time: 4.38 s\n",
      "Loss: 1.024, Train acc: 0.994, Test acc: 0.820\n",
      "Trial 26/30... Training time: 5.27 s\n",
      "Loss: 1.299, Train acc: 0.985, Test acc: 0.804\n",
      "Trial 27/30... Training time: 4.43 s\n",
      "Loss: 0.639, Train acc: 0.996, Test acc: 0.816\n",
      "Trial 28/30... Training time: 5.99 s\n",
      "Loss: 0.482, Train acc: 0.998, Test acc: 0.828\n",
      "Trial 29/30... Training time: 4.19 s\n",
      "Loss: 0.829, Train acc: 0.998, Test acc: 0.817\n",
      "Trial 30/30... Training time: 4.36 s\n",
      "Loss: 1.349, Train acc: 0.984, Test acc: 0.818\n"
     ]
    }
   ],
   "source": [
    "all_losses, tr_accuracies, te_accuracies = trial(Net(64, verbose=False), train_loader, test_loader, \n",
    "                                                 n_trials=30, alpha=0, alpha_decay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9982),\n",
       " tensor(0.0024),\n",
       " tensor(0.8180),\n",
       " tensor(0.0064),\n",
       " tensor(0.8170))"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_accuracies.mean(), tr_accuracies.std(), te_accuracies.mean(), te_accuracies.std(), te_accuracies.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(te_accuracies)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
